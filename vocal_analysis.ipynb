{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "import os\n",
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repetition_wav_files = [os.path.join('audio samples/disfluency/repetition', f) for f in os.listdir('./audio samples/disfluency/repetition')]\n",
    "stutter_wav_files = [os.path.join('audio samples/disfluency/stutter', f) for f in os.listdir('./audio samples/disfluency/stutter')]\n",
    "pause_wav_files = [os.path.join('audio samples/disfluency/pause', f) for f in os.listdir('./audio samples/disfluency/pause')]\n",
    "stutter_pause_wav_files = [os.path.join('audio samples/disfluency/stutter_pause', f) for f in os.listdir('./audio samples/disfluency/stutter_pause')]\n",
    "noise_wav_files = [os.path.join('audio samples/disfluency/noise', f) for f in os.listdir('./audio samples/disfluency/noise')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Repetitions using Google "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiаlize  reсоgnizer  сlаss  (fоr  reсоgnizing  the  sрeeсh)\n",
    "r = sr.Recognizer()\n",
    "sentence = \"\"\n",
    "for audio in repetition_wav_files:\n",
    "    #Use  the  reсоgnize_google()  funсtiоn  tо  reсоgnize  the  аudiо\n",
    "    with sr.AudioFile(audio) as source:\n",
    "        # print('Say  something!')\n",
    "        audio = r.record(source)\n",
    "        # print('Done!')\n",
    "    try:\n",
    "        sentence = r.recognize_google(audio)\n",
    "        print('You  said :  ' + sentence)\n",
    "    except Exception as  e:\n",
    "        print('Error:  ' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Function to split the text into words\n",
    "Counter(sentence.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 1 : Feature Engineering in audio files (chatGPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_disfluencies_files = repetition_wav_files + stutter_wav_files + pause_wav_files + stutter_pause_wav_files + noise_wav_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spectrogram(audio_file):\n",
    "    # Load audio file\n",
    "    y, sr = librosa.load(audio_file)\n",
    "    \n",
    "    # Generate spectrogram\n",
    "    spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "    \n",
    "    # Convert to dB scale\n",
    "    spectrogram_db = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "    \n",
    "    # Plot spectrogram\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(spectrogram_db, sr=sr, x_axis='time', y_axis='mel')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Spectrogram')\n",
    "    plt.show()\n",
    "\n",
    "# put the above cell in one function \n",
    "def plot_wav(filename):\n",
    "    # read the file\n",
    "    samplerate, data = wavfile.read(filename)\n",
    "    # get the duration\n",
    "    duration = len(data)/samplerate\n",
    "    # create a time variable\n",
    "    time = np.arange(0,duration,1/samplerate)\n",
    "    # plot amplitude (or loudness) over time\n",
    "    plt.plot(time,data)\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Amplitude')\n",
    "    # put a label with the file name \n",
    "    plt.title(filename)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for audio_file in all_disfluencies_files:\n",
    "    plot_wav(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for audio_file in all_disfluencies_files:    \n",
    "    y, sr = librosa.load(audio_file)\n",
    "    # Extract energy feature\n",
    "    energy = np.max(librosa.feature.rms(y=y))\n",
    "    # Extract pitch feature\n",
    "    pitches, _ = librosa.piptrack(y=y, sr=sr)\n",
    "    pitch_mean = np.mean(pitches[pitches > 0])\n",
    "    print(f\"{energy=} , {pitch_mean=} , for {audio_file=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract energy and pitch features from audio file\n",
    "def extract_features(audio_file):\n",
    "    y, sr = librosa.load(audio_file)\n",
    "    \n",
    "    # Extract energy feature\n",
    "    energy = np.mean(librosa.feature.rms(y=y))\n",
    "    \n",
    "    # Extract pitch feature\n",
    "    pitches, _ = librosa.piptrack(y=y, sr=sr)\n",
    "    pitch_mean = np.mean(pitches[pitches > 0])\n",
    "    \n",
    "    return [energy, pitch_mean]\n",
    "\n",
    "# Example usage: Extract features from audio files in a dataset\n",
    "def extract_features_from_dataset(audio_files):\n",
    "    features = []\n",
    "    for file in audio_files:\n",
    "        features.append(extract_features(file))\n",
    "    return np.array(features)\n",
    "\n",
    "def extract_mfcc(audio_file):\n",
    "    y, sr = librosa.load(audio_file)\n",
    "    # Extract MFCCs\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    return mfcc\n",
    "\n",
    "def extract_mfcc_from_dataset(audio_files):\n",
    "    mfccs = []\n",
    "    for file in audio_files:\n",
    "        mfccs.append(extract_mfcc(file))\n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dataset of audio files and corresponding labels\n",
    "all_disfluencies_files = stutter_wav_files + pause_wav_files + stutter_pause_wav_files + noise_wav_files\n",
    "labels = ['stutter'] * len(stutter_wav_files) + ['pause'] * len(pause_wav_files) + ['st_p'] * len(stutter_pause_wav_files) + ['noise'] * len(noise_wav_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (file,label) in zip(all_disfluencies_files,labels):\n",
    "    print(f\"{file=} , {label=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dataset of audio files and corresponding labels\n",
    "all_disfluencies_files = stutter_wav_files + pause_wav_files + stutter_pause_wav_files + noise_wav_files\n",
    "labels = ['stutter'] * len(stutter_wav_files) + ['pause'] * len(pause_wav_files) + ['st_p'] * len(stutter_pause_wav_files) + ['noise'] * len(noise_wav_files)\n",
    "\n",
    "# Extract features from the dataset\n",
    "X = extract_mfcc_from_dataset(all_disfluencies_files)\n",
    "\n",
    "# Convert labels to numerical format\n",
    "label_mapping = {'stutter': 1, 'pause': 2, 'st_p': 3, 'noise': 4}\n",
    "y = np.array([label_mapping[label] for label in labels])\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the data\n",
    "X_train_flat = [mfcc.flatten() for mfcc in X_train]\n",
    "X_test_flat = [mfcc.flatten() for mfcc in X_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest classifier on the mfcc features\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-TF2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
