{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.io import wavfile\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "repetition_wav_files = [os.path.join('audio samples/disfluency/repetition', f) for f in os.listdir('./audio samples/disfluency/repetition')]\n",
    "stutter_wav_files = [os.path.join('audio samples/disfluency/stutter', f) for f in os.listdir('./audio samples/disfluency/stutter')]\n",
    "pause_wav_files = [os.path.join('audio samples/disfluency/pause', f) for f in os.listdir('./audio samples/disfluency/pause')]\n",
    "stutter_pause_wav_files = [os.path.join('audio samples/disfluency/stutter_pause', f) for f in os.listdir('./audio samples/disfluency/stutter_pause')]\n",
    "noise_wav_files = [os.path.join('audio samples/disfluency/noise', f) for f in os.listdir('./audio samples/disfluency/noise')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Repetitions using Google "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiаlize  reсоgnizer  сlаss  (fоr  reсоgnizing  the  sрeeсh)\n",
    "r = sr.Recognizer()\n",
    "sentence = \"\"\n",
    "for audio in repetition_wav_files:\n",
    "    #Use  the  reсоgnize_google()  funсtiоn  tо  reсоgnize  the  аudiо\n",
    "    with sr.AudioFile(audio) as source:\n",
    "        # print('Say  something!')\n",
    "        audio = r.record(source)\n",
    "        # print('Done!')\n",
    "    try:\n",
    "        sentence = r.recognize_google(audio)\n",
    "        print('You  said :  ' + sentence)\n",
    "    except Exception as  e:\n",
    "        print('Error:  ' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Function to split the text into words\n",
    "Counter(sentence.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 1 : Feature Engineering in audio files (chatGPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_disfluencies_files = repetition_wav_files + stutter_wav_files + pause_wav_files + stutter_pause_wav_files + noise_wav_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spectrogram(audio_file):\n",
    "    # Load audio file\n",
    "    y, sr = librosa.load(audio_file)\n",
    "    \n",
    "    # Generate spectrogram\n",
    "    spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "    \n",
    "    # Convert to dB scale\n",
    "    spectrogram_db = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "    \n",
    "    # Plot spectrogram\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(spectrogram_db, sr=sr, x_axis='time', y_axis='mel')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Spectrogram')\n",
    "    plt.show()\n",
    "\n",
    "# put the above cell in one function \n",
    "def plot_wav(filename):\n",
    "    # read the file\n",
    "    samplerate, data = wavfile.read(filename)\n",
    "    # get the duration\n",
    "    duration = len(data)/samplerate\n",
    "    # create a time variable\n",
    "    time = np.arange(0,duration,1/samplerate)\n",
    "    # plot amplitude (or loudness) over time\n",
    "    plt.plot(time,data)\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Amplitude')\n",
    "    # put a label with the file name \n",
    "    plt.title(filename)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for audio_file in all_disfluencies_files:\n",
    "    plot_wav(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for audio_file in all_disfluencies_files:    \n",
    "    y, sr = librosa.load(audio_file)\n",
    "    # Extract energy feature\n",
    "    energy = np.max(librosa.feature.rms(y=y))\n",
    "    # Extract pitch feature\n",
    "    pitches, _ = librosa.piptrack(y=y, sr=sr)\n",
    "    pitch_mean = np.mean(pitches[pitches > 0])\n",
    "    print(f\"{energy=} , {pitch_mean=} , for {audio_file=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract energy and pitch features from audio file\n",
    "def extract_features(audio_file):\n",
    "    y, sr = librosa.load(audio_file)\n",
    "    \n",
    "    # Extract energy feature\n",
    "    energy = np.mean(librosa.feature.rms(y=y))\n",
    "    \n",
    "    # Extract pitch feature\n",
    "    pitches, _ = librosa.piptrack(y=y, sr=sr)\n",
    "    pitch_mean = np.mean(pitches[pitches > 0])\n",
    "    \n",
    "    return [energy, pitch_mean]\n",
    "\n",
    "# Example usage: Extract features from audio files in a dataset\n",
    "def extract_features_from_dataset(audio_files):\n",
    "    features = []\n",
    "    for file in audio_files:\n",
    "        features.append(extract_features(file))\n",
    "    return np.array(features)\n",
    "\n",
    "def extract_mfcc(audio_file):\n",
    "    y, sr = librosa.load(audio_file)\n",
    "    # Extract MFCCs\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    return mfcc\n",
    "\n",
    "def extract_mfcc_from_dataset(audio_files):\n",
    "    mfccs = []\n",
    "    for file in audio_files:\n",
    "        mfccs.append(extract_mfcc(file))\n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dataset of audio files and corresponding labels\n",
    "labels = ['stutter'] * len(stutter_wav_files) + ['pause'] * len(pause_wav_files) + ['st_p'] * len(stutter_pause_wav_files) + ['noise'] * len(noise_wav_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (file,label) in zip(all_disfluencies_files,labels):\n",
    "    print(f\"{file=} , {label=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dataset of audio files and corresponding labels\n",
    "\n",
    "# Extract features from the dataset\n",
    "X = extract_mfcc_from_dataset(all_disfluencies_files)\n",
    "\n",
    "# Convert labels to numerical format\n",
    "label_mapping = {'stutter': 1, 'pause': 2, 'st_p': 3, 'noise': 4}\n",
    "y = np.array([label_mapping[label] for label in labels])\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the data\n",
    "X_train_flat = [mfcc.flatten() for mfcc in X_train]\n",
    "X_test_flat = [mfcc.flatten() for mfcc in X_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest classifier on the mfcc features\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to extract MFCC features from audio files\n",
    "def extract_mfcc(audio_path, max_pad_len=100):\n",
    "    # Load audio file\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    # Extract MFCC features\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr)\n",
    "    # Pad or truncate MFCC features to a fixed length\n",
    "    if mfccs.shape[1] < max_pad_len:\n",
    "        pad_width = max_pad_len - mfccs.shape[1]\n",
    "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        mfccs = mfccs[:, :max_pad_len]\n",
    "    return mfccs\n",
    "\n",
    "# Extract MFCC features for all audio files\n",
    "max_pad_len = 100  # Maximum length of MFCC features\n",
    "X = [extract_mfcc(audio_file, max_pad_len=max_pad_len) for audio_file in all_disfluencies_files]\n",
    "\n",
    "# Convert list to numpy array\n",
    "X = np.array(X)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Flatten MFCC features\n",
    "X_train_flatten = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flatten = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Train Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train_flatten, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "accuracy = clf.score(X_test_flatten, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windowing Approach 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'audio samples/disfluency/repetition\\\\repetition_01.wav'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_disfluencies_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio file\n",
    "audio_path = all_disfluencies_files[0]\n",
    "y, sr = librosa.load(audio_path, sr=None)\n",
    "\n",
    "# Define parameters for windowing and feature extraction\n",
    "frame_size = 2048  # Size of each frame in samples\n",
    "hop_length = 512   # Hop length (frame overlap) in samples\n",
    "n_mfcc = 13        # Number of MFCC coefficients to extract\n",
    "\n",
    "# Extract MFCC features using frame-wise processing\n",
    "mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, n_fft=frame_size, hop_length=hop_length)\n",
    "\n",
    "# Plot MFCC features\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(mfccs, sr=sr, hop_length=hop_length, x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title('MFCC')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "uclass_annotation_csv_files = [os.path.join('audio samples/uclass_dataset/uclass_annotations', f) for f in os.listdir('./audio samples/uclass_dataset/uclass_annotations')]\n",
    "uclass_audio_files = [os.path.join('audio samples/uclass_dataset', f) for f in os.listdir('./audio samples/uclass_dataset') if f.endswith('.wav')]\n",
    "\n",
    "assert len(uclass_annotation_csv_files) == len(uclass_audio_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Column 8:\tLabel classifying utterance as clean speech, or one of 6 stutter types._\n",
    "-\t0: \tclean\n",
    "-\t1: \tinterjection\n",
    "-\t2: \tsound repetition\n",
    "-\t3: \tpart-word repetition\n",
    "-\t4: \tword repetition\n",
    "-\t5: \tphrase repetition\n",
    "-\t6: \trevision\n",
    "-\t7: \tprolongation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_label_from_csv(csv_file):\n",
    "    df = pd.read_csv(csv_file, header=None, encoding='utf-8')\n",
    "    df = df[df[7] != 0]\n",
    "    category = df[7].value_counts().idxmax()\n",
    "    if category == 5:\n",
    "        category = 2\n",
    "    elif category == 6:\n",
    "        category = 1\n",
    "    return category\n",
    "\n",
    "\n",
    "def map_audiofiles_to_labels(audio_files, annotation_files):\n",
    "    audio_to_labels = {}\n",
    "    for audio_file, annotation_file in product(uclass_audio_files,uclass_annotation_csv_files):\n",
    "        audio_name = audio_file.split('/')[-1].split('\\\\')[-1].split('.')[0]\n",
    "        annotation_name = annotation_file.split('/')[-1].split('\\\\')[-1].split('.')[0]\n",
    "        if audio_name == annotation_name:\n",
    "            try:\n",
    "                audio_to_labels[audio_file] = extract_label_from_csv(annotation_file)\n",
    "            except Exception as e:\n",
    "                print(f\"Error Reading Filename: {annotation_file}\")\n",
    "                continue\n",
    "    return audio_to_labels\n",
    "\n",
    "def extract_mfcc_with_window(audio_and_labels, window_size=2048, hop_length=512, n_mfcc=13, max_pad_len=100):\n",
    "    mfccs = []\n",
    "    labels = []\n",
    "    for audio_file, label in audio_and_labels.items():\n",
    "        y, sr = librosa.load(audio_file, sr=None)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, n_fft=window_size, hop_length=hop_length)\n",
    "        # pad mfccs to a fixed length\n",
    "        if mfcc.shape[1] < max_pad_len:\n",
    "            pad_width = max_pad_len - mfccs.shape[1]\n",
    "            mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "        else:\n",
    "            mfcc = mfcc[:, :max_pad_len]\n",
    "        mfccs.append(mfcc)\n",
    "        labels.append(label)\n",
    "    return np.array(mfccs) , np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_and_labels = map_audiofiles_to_labels(uclass_audio_files, uclass_annotation_csv_files)\n",
    "# 1: stutter and pause , 2: repetition\n",
    "X, y = extract_mfcc_with_window(audio_and_labels, window_size=2048, hop_length=512, n_mfcc=100, max_pad_len=40)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "# we use cross-validation as the length is small \n",
    "# Flatten MFCC features\n",
    "X_train_flatten = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flatten = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Train Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "clf.fit(X_train_flatten, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "accuracy = clf.score(X_test_flatten, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "clf.predict(X_test_flatten[0].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = {stutter_pause_wav_files[0] : 1}\n",
    "mfcc, label = extract_mfcc_with_window(test, window_size=2048, hop_length=512, n_mfcc=100, max_pad_len=40)\n",
    "clf.predict(mfcc.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JUST A CELL FOR GETTING COMMON FILES and DELETING THE REST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio samples/uclass_dataset\\\\F_0101_10y4m_1.wav',\n",
       " 'audio samples/uclass_dataset\\\\F_0101_13y1m_1.wav',\n",
       " 'audio samples/uclass_dataset\\\\M_0028_15y11m_1.wav',\n",
       " 'audio samples/uclass_dataset\\\\M_0030_12y1m_1.wav',\n",
       " 'audio samples/uclass_dataset\\\\M_0030_17y9m_1.wav',\n",
       " 'audio samples/uclass_dataset\\\\M_0052_16y4m_1.wav',\n",
       " 'audio samples/uclass_dataset\\\\M_0061_14y8m_1.wav',\n",
       " 'audio samples/uclass_dataset\\\\M_0078_12y4m_1.wav',\n",
       " 'audio samples/uclass_dataset\\\\M_0078_16y5m_1.wav',\n",
       " 'audio samples/uclass_dataset\\\\M_0095_07y7m_1.wav',\n",
       " 'audio samples/uclass_dataset\\\\M_0095_08y10m_1.wav',\n",
       " 'audio samples/uclass_dataset\\\\M_0098_07y8m_1.wav',\n",
       " 'audio samples/uclass_dataset\\\\M_0098_09y8m_1.wav',\n",
       " 'audio samples/uclass_dataset\\\\M_0098_10y6m_1.wav',\n",
       " 'audio samples/uclass_dataset\\\\M_0100_11y2m_1.wav',\n",
       " 'audio samples/uclass_dataset\\\\M_0100_12y3m_1.wav',\n",
       " 'audio samples/uclass_dataset\\\\M_0100_13y10m_1.wav',\n",
       " 'audio samples/uclass_dataset\\\\M_0138_12y2m_1.wav',\n",
       " 'audio samples/uclass_dataset\\\\M_0138_13y3m_1.wav',\n",
       " 'audio samples/uclass_dataset\\\\M_0210_11y3m_1.wav',\n",
       " 'audio samples/uclass_dataset\\\\M_0219_11y2m_1.wav',\n",
       " 'audio samples/uclass_dataset\\\\M_0234_09y9m_1.wav',\n",
       " 'audio samples/uclass_dataset\\\\M_0394_09y2m_1.wav',\n",
       " 'audio samples/uclass_dataset\\\\M_0394_09y5m_1.wav',\n",
       " 'audio samples/uclass_dataset\\\\M_0556_08y0m_1.wav'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# common_files = set()\n",
    "# for audio_file, annotation_file in product(uclass_audio_files,uclass_annotation_csv_files):\n",
    "#     audio_name = audio_file.split('/')[-1].split('\\\\')[-1].split('.')[0]\n",
    "#     annotation_name = annotation_file.split('/')[-1].split('\\\\')[-1].split('.')[0]\n",
    "#     if audio_name == annotation_name:\n",
    "#         common_files.add(audio_file)\n",
    "\n",
    "# common_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-TF2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
